+++
+++

### Software Developer, J.P. Morgan Chase & Co  
At J.P. Morgan Chase & Co, I designed and implemented data pipelines for a Multi-Touch Attribution model, processing over 3 TB of customer data to optimize $2M+ in ad spend. I orchestrated complex workflows using AWS Step Functions, integrating Lambda functions, EMR clusters, and AWS Glue. By fine-tuning PySpark jobs and leveraging spot instances, I saved $50k in computing costs. Additionally, I streamlined CI/CD pipelines using Docker and Terraform, reducing deployment time for critical applications. I also proposed and implemented Snowflake-based solutions for a centralized data lake, improving query performance by 35%.

### Software Developer, Wells Fargo  
I developed REST APIs using Flask and SQLAlchemy to migrate credit card transaction data from legacy systems to Hadoop, enabling faster data processing for business teams. I optimized relational database schemas and implemented secure authentication using OAuth 2.0 and JWT. By leveraging MongoDB for unstructured data storage and improving SQL query performance with Hive partitioning, I achieved a 45% performance boost. I also conducted API testing using Postman and Pytest, ensuring robust and secure integrations.

### Programmer Analyst, Cognizant Technology Solutions  
I engineered CECL/CCAR regulatory reporting pipelines, processing over 1 TB of monthly financial data to ensure compliance and accurate loss forecasting. By optimizing PySpark SQL queries using broadcast joins and predicate pushdown, I reduced query execution time by 70%. I automated deployment processes for 20+ PySpark jobs using shell scripts, reducing manual intervention by 80%. Additionally, I provided technical support during the transition to Hadoop 3.x, improving system stability by 50%.